{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMewtV7Ma7k+hRbtcX8hG2x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"efebb6cf"},"source":["%pip install torch transformers datasets evaluate accelerate bitsandbytes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load latest transformer package"],"metadata":{"id":"mRsowW4w35Ph"}},{"cell_type":"code","source":["%pip install git+https://github.com/huggingface/transformers.git"],"metadata":{"id":"wWO7vocJTehv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7CruHa2Fo08"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","import torch\n","from tqdm.auto import tqdm # Import tqdm\n","import pandas as pd\n","import evaluate\n","import os"]},{"cell_type":"code","source":["import gc\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"4D0RaUMnAWDm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6f1d1278"},"source":["## Download Model\n","\n","Download a pre-trained CUDA-LLM model.\n","Use the `transformers` library to download a pre-trained language model."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nKFROW8fSeu1"},"outputs":[],"source":["checkpoint = \"ByteDance-Seed/cudaLLM-8B\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float16).to(device)"]},{"cell_type":"markdown","metadata":{"id":"9354ff9d"},"source":["Download a CUDA-LLM model and the NVIDIA compute-eval dataset, then evaluate the model against the dataset and report the results.\n","Gated repo - needs access permissions on Huggingface."]},{"cell_type":"code","metadata":{"id":"b2c30424"},"source":["from datasets import load_dataset\n","\n","dataset_name = \"nvidia/compute-eval\" # The name of the dataset\n","\n","try:\n","    dataset = load_dataset(dataset_name)\n","    print(f\"Successfully downloaded dataset: {dataset_name}\")\n","    print(dataset)\n","except Exception as e:\n","    print(f\"Error downloading dataset: {e}\")\n","    print(\"Please ensure you have the correct dataset name and are connected to the internet.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0ff88c5"},"source":["## Alternatevely download Dataset from GitHub\n","Clone the NVIDIA compute-eval repository from GitHub.\n"]},{"cell_type":"code","metadata":{"id":"cb0bcb59"},"source":["!git clone https://github.com/nvidia/compute-eval.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cddb457a"},"source":["## Generate Solutions and Save\n","\n","Generate solutions for the compute-eval dataset using the loaded model and save them to a JSONL file.\n","\n","Iterate through the dataset, run inference with the loaded model for each input, and save the generated solutions in a JSONL file."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9e4ec09f","executionInfo":{"status":"ok","timestamp":1761486399619,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sandeepa Prabhu","userId":"01938514220573794641"}},"outputId":"3d26bd0a-80ef-411a-fd0f-d9bd4af9a6ac"},"source":["import os\n","\n","# List files in the cloned repository directory\n","repo_dir = \"compute-eval\" # The name of the cloned directory\n","print(f\"Contents of {repo_dir}:\")\n","print(os.listdir(repo_dir))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Contents of compute-eval:\n","['example_config_gen_samples.yaml', '.git', 'data', 'README.md', 'compute_eval', '.DS_Store', 'pyproject.toml', '.gitignore', 'DATASET_CARD.md', 'CONTRIBUTING.md', 'LICENSE', 'example_config_evalcorrectness.yaml', 'poetry.lock']\n"]}]},{"cell_type":"code","metadata":{"id":"e4a6d65d"},"source":["from datasets import load_dataset\n","import os\n","\n","data_dir = os.path.join(\"compute-eval\", \"data\")\n","dataset_file_path = os.path.join(data_dir, 'cuda_problems_073025.jsonl')\n","\n","try:\n","    dataset = load_dataset('json', data_files=dataset_file_path)\n","    print(\"\\nSuccessfully loaded dataset from file:\")\n","    print(dataset)\n","except FileNotFoundError:\n","    print(f\"\\nDataset file not found at {dataset_file_path}. Please check the path.\")\n","except Exception as e:\n","    print(f\"\\nError loading dataset from file: {e}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8a40922"},"source":["import os\n","\n","data_dir = os.path.join(\"compute-eval\", \"data\")\n","print(f\"Contents of {data_dir}:\")\n","print(os.listdir(data_dir))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"502f1368"},"source":["import json\n","import os\n","\n","output_file = \"compute_eval_solutions.jsonl\"\n","solutions = []\n","count = 0 # Initialize a counter\n","output_dir = \"cuda\" # Define the output directory\n","\n","# Create the output directory if it doesn't exist\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Ensure the model is in evaluation mode\n","model.eval()\n","\n","with torch.no_grad(): # Disable gradient calculation for inference\n","    for item in tqdm(dataset['train'], desc=\"Generating Solutions\"): # Assuming 'train' split, adjust if needed\n","        #if count >= 10: # Exit loop after 3 solutions\n","        #    break\n","\n","        prompt = item['prompt'] # Assuming 'prompt' is the key for input text\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","        # Generate response\n","        # You may need to adjust generation parameters like max_length, num_beams, etc.\n","        outputs = model.generate(**inputs, max_length=1024, num_beams=5, early_stopping=True) # Increased max_length\n","\n","        # Decode the generated text\n","        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Extract the solution from the generated text (this might require custom parsing)\n","        # For simplicity, we'll assume the generated text is the solution for now\n","        solution = generated_text\n","\n","        solutions.append({\"prompt\": prompt, \"solution\": solution})\n","\n","        # Save the solution to a .cu file\n","        solution_filename = os.path.join(output_dir, f\"solution_{count}.cu\")\n","        with open(solution_filename, 'w') as f:\n","            f.write(solution)\n","\n","        count += 1 # Increment the counter\n","\n","# Save solutions to a JSONL file (optional, as solutions are also saved as .cu files)\n","with open(output_file, 'w') as f:\n","    for entry in solutions:\n","        json.dump(entry, f)\n","        f.write('\\n')\n","\n","print(f\"\\nGenerated solutions saved to {output_file} and individual .cu files in the '{output_dir}' directory.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8d812c84"},"source":["## Print Generated Solutions from File\n","\n","Open and read the JSON Lines file containing the generated solutions and print each entry."]},{"cell_type":"code","metadata":{"id":"9cbdeaa5"},"source":["import json\n","\n","output_file = \"compute_eval_solutions.jsonl\"\n","\n","try:\n","    with open(output_file, 'r') as f:\n","        print(f\"Contents of {output_file}:\")\n","        for line in f:\n","            solution_entry = json.loads(line)\n","            print(solution_entry)\n","except FileNotFoundError:\n","    print(f\"Error: The file {output_file} was not found.\")\n","except json.JSONDecodeError:\n","    print(f\"Error: Could not decode JSON from {output_file}. Ensure it's a valid JSON Lines file.\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"mdC6UCcz5iBh"}},{"cell_type":"code","metadata":{"id":"e4ff906c"},"source":["import os\n","\n","repo_dir = \"compute-eval\"\n","print(f\"Contents of {repo_dir}:\")\n","print(os.listdir(repo_dir))\n","\n","# Check for common documentation files and directories\n","docs_files = [\"README.md\", \"DATASET_CARD.md\", \"CONTRIBUTING.md\"]\n","eval_dirs = [\"scripts\", \"eval\"]\n","\n","print(\"\\nChecking for documentation files:\")\n","for doc_file in docs_files:\n","    if os.path.exists(os.path.join(repo_dir, doc_file)):\n","        print(f\"- Found: {doc_file}\")\n","    else:\n","        print(f\"- Not found: {doc_file}\")\n","\n","print(\"\\nChecking for evaluation directories:\")\n","for eval_dir in eval_dirs:\n","    if os.path.exists(os.path.join(repo_dir, eval_dir)):\n","        print(f\"- Found: {eval_dir}\")\n","        # List contents of found evaluation directories\n","        print(f\"  Contents of {os.path.join(repo_dir, eval_dir)}:\")\n","        try:\n","            print(os.listdir(os.path.join(repo_dir, eval_dir)))\n","        except NotADirectoryError:\n","            print(f\"  {os.path.join(repo_dir, eval_dir)} is not a directory.\")\n","    else:\n","        print(f\"- Not found: {eval_dir}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9306a4c"},"source":["import os\n","\n","repo_dir = \"compute-eval\"\n","docs_files = [\"README.md\", \"DATASET_CARD.md\", \"CONTRIBUTING.md\"]\n","\n","print(\"Reading documentation files:\")\n","for doc_file in docs_files:\n","    file_path = os.path.join(repo_dir, doc_file)\n","    if os.path.exists(file_path):\n","        print(f\"\\n--- Contents of {doc_file} ---\")\n","        try:\n","            with open(file_path, 'r') as f:\n","                print(f.read())\n","        except Exception as e:\n","            print(f\"Error reading {doc_file}: {e}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b94beed7"},"source":["import os\n","\n","repo_dir = \"compute-eval\"\n","eval_config_file = \"example_config_evalcorrectness.yaml\"\n","file_path = os.path.join(repo_dir, eval_config_file)\n","\n","print(f\"\\n--- Contents of {eval_config_file} ---\")\n","try:\n","    with open(file_path, 'r') as f:\n","        print(f.read())\n","except FileNotFoundError:\n","    print(f\"Error: The file {eval_config_file} was not found.\")\n","except Exception as e:\n","    print(f\"Error reading {eval_config_file}: {e}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ebac1e97"},"source":["import os\n","\n","repo_dir = \"compute-eval\"\n","compute_eval_dir = os.path.join(repo_dir, \"compute_eval\")\n","\n","print(f\"\\nContents of {compute_eval_dir}:\")\n","try:\n","    print(os.listdir(compute_eval_dir))\n","    # Check for files that might indicate evaluation logic (e.g., eval.py, correctness.py)\n","    print(\"\\nChecking for potential evaluation files in compute_eval directory:\")\n","    for root, _, files in os.walk(compute_eval_dir):\n","        for file in files:\n","            if file.endswith(\".py\") and (\"eval\" in file or \"correctness\" in file or \"metric\" in file):\n","                print(f\"- Found: {os.path.join(root, file)}\")\n","except FileNotFoundError:\n","    print(f\"Error: The directory {compute_eval_dir} was not found.\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44e9831a"},"source":["## Prepare environment for compilation/execution\n","\n","Ensure the environment has the necessary CUDA toolkit and compilers to build and run the generated code.\n"]},{"cell_type":"markdown","metadata":{"id":"285db221"},"source":["Check for the availability of `nvcc` and `g++` compilers using shell commands as instructed to ensure the environment is set up for compiling CUDA code.\n","\n"]},{"cell_type":"code","metadata":{"id":"e13bcc69"},"source":["import os\n","\n","# Check for nvcc\n","print(\"Checking for nvcc compiler:\")\n","nvcc_check = os.system(\"nvcc --version\")\n","if nvcc_check != 0:\n","    print(\"nvcc not found. CUDA Toolkit may need to be installed or configured.\")\n","else:\n","    print(\"nvcc found.\")\n","\n","# Check for g++\n","print(\"\\nChecking for g++ compiler:\")\n","gpp_check = os.system(\"g++ --version\")\n","if gpp_check != 0:\n","    print(\"g++ not found. A C++ compiler may need to be installed.\")\n","else:\n","    print(\"g++ found.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69c849d6"},"source":["## Extract test cases\n","\n"]},{"cell_type":"markdown","metadata":{"id":"747e25dc"},"source":["Extract the test cases from the 'test' column of the 'train' split of the loaded dataset and store them in a list.\n","\n"]},{"cell_type":"code","metadata":{"id":"4ea7ce46"},"source":["test_cases = [item['test'] for item in dataset['train']]\n","print(f\"Extracted {len(test_cases)} test cases.\")\n","# Optionally, print the first few test cases to verify\n","# print(\"First 5 test cases:\")\n","# for i, test_case in enumerate(test_cases[:5]):\n","#     print(f\"--- Test Case {i+1} ---\")\n","#     print(test_case)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2905986"},"source":["## Implement evaluation logic\n","\n","Implement evaluation logic to iterate through generated solutions and test cases, compile and execute the combined code, and capture the results.\n"]},{"cell_type":"code","metadata":{"id":"57730c2e"},"source":["import subprocess\n","import os\n","import json\n","\n","output_file = \"compute_eval_solutions.jsonl\"\n","eval_results = []\n","num_solutions_to_eval = 3 # Evaluate the first 3 generated solutions\n","\n","# Load generated solutions\n","try:\n","    with open(output_file, 'r') as f:\n","        generated_solutions = [json.loads(line) for line in f]\n","except FileNotFoundError:\n","    print(f\"Error: The file {output_file} was not found.\")\n","    generated_solutions = []\n","except json.JSONDecodeError:\n","    print(f\"Error: Could not decode JSON from {output_file}. Ensure it's a valid JSON Lines file.\")\n","    generated_solutions = []\n","except Exception as e:\n","    print(f\"An error occurred while loading generated solutions: {e}\")\n","    generated_solutions = []\n","\n","\n","for i in tqdm(range(min(num_solutions_to_eval, len(generated_solutions))), desc=\"Evaluating Solutions\"):\n","    solution_entry = generated_solutions[i]\n","    # Find the corresponding dataset entry based on prompt (assuming prompts are unique and ordered)\n","    # A more robust approach would be to use task_id if available in the solution file\n","    dataset_entry = None\n","    for item in dataset['train']:\n","        if item['prompt'] == solution_entry['prompt']:\n","            dataset_entry = item\n","            break\n","\n","    if dataset_entry is None:\n","        print(f\"Warning: Could not find dataset entry for solution {i+1}. Skipping.\")\n","        continue\n","\n","    task_id = dataset_entry.get('task_id', f\"Task {i+1}\")\n","    prompt = dataset_entry['prompt']\n","    solution = solution_entry['solution']\n","    declaration = dataset_entry.get('declaration', '')\n","    test = dataset_entry['test']\n","    cc_flags = dataset_entry.get('cc_flags', '')\n","    ld_flags = dataset_entry.get('ld_flags', '')\n","\n","    # Combine code\n","    combined_code = f\"{declaration}\\n{solution}\\n{test}\"\n","\n","    # Save combined code to a temporary file\n","    temp_file_name = f\"temp_code_{i}.cu\"\n","    with open(temp_file_name, 'w') as f:\n","        f.write(combined_code)\n","\n","    compile_success = False\n","    compile_output = \"\"\n","    exec_success = False\n","    exec_output = \"\"\n","\n","    # Compile the code\n","    compile_command = [\"nvcc\", temp_file_name, \"-o\", f\"temp_output_{i}\", *cc_flags.split(), *ld_flags.split()]\n","    try:\n","        compile_result = subprocess.run(compile_command, capture_output=True, text=True, timeout=30) # Added timeout\n","        compile_output = compile_result.stdout + compile_result.stderr\n","        if compile_result.returncode == 0:\n","            compile_success = True\n","            # Execute the compiled binary if compilation was successful\n","            execute_command = [f\"./temp_output_{i}\"]\n","            try:\n","                exec_result = subprocess.run(execute_command, capture_output=True, text=True, timeout=30) # Added timeout\n","                exec_output = exec_result.stdout + exec_result.stderr\n","                if exec_result.returncode == 0:\n","                    exec_success = True\n","            except FileNotFoundError:\n","                exec_output = f\"Error: Executable ./temp_output_{i} not found.\"\n","            except subprocess.TimeoutExpired:\n","                exec_output = \"Execution timed out.\"\n","            except Exception as e:\n","                exec_output = f\"Error during execution: {e}\"\n","        else:\n","            compile_output = f\"Compilation failed with return code {compile_result.returncode}:\\n\" + compile_output\n","    except FileNotFoundError:\n","        compile_output = \"Error: nvcc command not found. Is CUDA Toolkit installed and in PATH?\"\n","    except subprocess.TimeoutExpired:\n","        compile_output = \"Compilation timed out.\"\n","    except Exception as e:\n","        compile_output = f\"Error during compilation: {e}\"\n","\n","    # Store results\n","    eval_results.append({\n","        \"task_id\": task_id,\n","        \"compile_success\": compile_success,\n","        \"compile_output\": compile_output,\n","        \"exec_success\": exec_success,\n","        \"exec_output\": exec_output\n","    })\n","\n","    # Clean up temporary files\n","    if os.path.exists(temp_file_name):\n","        os.remove(temp_file_name)\n","    if os.path.exists(f\"temp_output_{i}\"):\n","        os.remove(f\"temp_output_{i}\")\n","\n","\n","print(\"\\nEvaluation Complete.\")\n","# Optionally, print the evaluation results\n","print(\"\\nEvaluation Results:\")\n","for result in eval_results:\n","    print(json.dumps(result, indent=2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6c0ff49"},"source":["## Calculate metrics\n","\n","Calculate evaluation metrics based on the compilation and execution results.\n"]},{"cell_type":"code","metadata":{"id":"53bd5fc8"},"source":["# Initialize counters\n","compile_success_count = 0\n","exec_success_count = 0\n","\n","# Iterate through the evaluation results\n","for result in eval_results:\n","    if result.get('compile_success', False):\n","        compile_success_count += 1\n","    if result.get('exec_success', False):\n","        exec_success_count += 1\n","\n","# Calculate pass rates\n","total_solutions_evaluated = len(eval_results)\n","compile_pass_rate = (compile_success_count / total_solutions_evaluated) * 100 if total_solutions_evaluated > 0 else 0\n","exec_pass_rate = (exec_success_count / total_solutions_evaluated) * 100 if total_solutions_evaluated > 0 else 0\n","\n","# Print the metrics\n","print(\"\\nEvaluation Metrics:\")\n","print(f\"Total solutions evaluated: {total_solutions_evaluated}\")\n","print(f\"Successful compilations: {compile_success_count}\")\n","print(f\"Successful executions: {exec_success_count}\")\n","print(f\"Compilation Pass Rate: {compile_pass_rate:.2f}%\")\n","print(f\"Execution Pass Rate: {exec_pass_rate:.2f}%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1ffa5aa"},"source":["## Report evaluation results\n","\n","Report the calculated evaluation metrics and provide examples of successful and failed evaluations.\n"]},{"cell_type":"code","metadata":{"id":"b57a981d"},"source":["# Print a summary of the evaluation\n","print(\"\\n--- Evaluation Summary ---\")\n","print(f\"Total solutions evaluated: {total_solutions_evaluated}\")\n","print(f\"Successful compilations: {compile_success_count}\")\n","print(f\"Successful executions: {exec_success_count}\")\n","print(f\"Compilation Pass Rate: {compile_pass_rate:.2f}%\")\n","print(f\"Execution Pass Rate: {exec_pass_rate:.2f}%\")\n","\n","print(\"\\n--- Evaluation Examples ---\")\n","\n","# Find and print examples of failed compilation and execution\n","failed_compile_example = None\n","failed_exec_example = None\n","successful_compile_example = None\n","successful_exec_example = None\n","\n","for result in eval_results:\n","    if not result.get('compile_success', True) and failed_compile_example is None:\n","        failed_compile_example = result\n","    if not result.get('exec_success', True) and failed_exec_example is None:\n","        failed_exec_example = result\n","    if result.get('compile_success', False) and successful_compile_example is None:\n","        successful_compile_example = result\n","    if result.get('exec_success', False) and successful_exec_example is None:\n","        successful_exec_example = result\n","\n","if failed_compile_example:\n","    print(\"\\nExample of a Failed Compilation:\")\n","    print(json.dumps(failed_compile_example, indent=2))\n","\n","if failed_exec_example and (failed_exec_example != failed_compile_example or not failed_compile_example): # Avoid printing the same example if it failed both\n","     print(\"\\nExample of a Failed Execution:\")\n","     print(json.dumps(failed_exec_example, indent=2))\n","\n","if successful_compile_example:\n","    print(\"\\nExample of a Successful Compilation:\")\n","    print(json.dumps(successful_compile_example, indent=2))\n","\n","if successful_exec_example and (successful_exec_example != successful_compile_example or not successful_compile_example): # Avoid printing the same example if it succeeded both\n","    print(\"\\nExample of a Successful Execution:\")\n","    print(json.dumps(successful_exec_example, indent=2))\n","\n","if not failed_compile_example and not failed_exec_example and not successful_compile_example and not successful_exec_example:\n","    print(\"No evaluation results available to display examples.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90472a85"},"source":["!zip -r cuda.zip cuda/"],"execution_count":null,"outputs":[]}]}