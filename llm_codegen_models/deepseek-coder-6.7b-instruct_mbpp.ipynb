{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"25c1750b"},"outputs":[],"source":["%pip install transformers accelerate\n","%pip install evaluate datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7CruHa2Fo08"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","import torch\n","from tqdm.auto import tqdm # Import tqdm\n","import pandas as pd\n","import evaluate\n","import os"]},{"cell_type":"code","source":["import gc\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"4D0RaUMnAWDm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4gw3El6Ft9e"},"outputs":[],"source":["# Load LLM\n","checkpoint = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.bfloat16).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gQAZuP1ng_b"},"outputs":[],"source":["inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n","outputs = model.generate(inputs)\n","print(tokenizer.decode(outputs[0]))"]},{"cell_type":"markdown","metadata":{"id":"f85152df"},"source":["## Load the humaneval dataset\n","\n","Load the HumanEval dataset using the `load_dataset` function from the `datasets` library.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"syCSYn9Nm73f"},"outputs":[],"source":["humaneval_dataset = load_dataset(\"mbpp\", \"sanitized\")"]},{"cell_type":"markdown","metadata":{"id":"1fe15b67"},"source":["## Generate solutions\n","\n","Iterate through the dataset and generate code solutions for each problem using the loaded CodeLlama model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"2929a542"},"outputs":[],"source":["generated_solutions = []\n","\n","# Number of samples to generate for each problem\n","num_samples = 15  # Reduce the number of samples to reduce memory usage\n","\n","for i, example in tqdm(enumerate(humaneval_dataset['test']), desc=\"Generating Solutions\", total=len(humaneval_dataset['test'])):\n","    prompt = \"# Complete the Python function below\\n\" + example['prompt']\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","    generated_ids = model.generate(\n","        inputs[\"input_ids\"],\n","        max_new_tokens=512, # Reduced max_new_tokens to reduce memory usage\n","        num_return_sequences=num_samples, # Generate num_samples sequences\n","        pad_token_id=tokenizer.eos_token_id,\n","        attention_mask=inputs[\"attention_mask\"], # Add attention mask\n","        do_sample=True # Enable sampling to generate multiple sequences\n","    )\n","\n","    decoded_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","    # Simple logic to extract the function for each generated output\n","    solutions_for_task = []\n","    for decoded_output in decoded_outputs:\n","        generated_code = \"\"\n","        start_index = decoded_output.find(\"def \")\n","        if start_index != -1:\n","            generated_code = decoded_output[start_index:]\n","            # Further refinement to stop at an empty line or other function definition could be added here\n","            # For simplicity, we take everything after 'def ') for now.\n","        solutions_for_task.append(generated_code)\n","\n","    generated_solutions.append({\n","        'task_id': example['task_id'],\n","        'prompt': example['prompt'],\n","        'solutions': solutions_for_task  # Store a list of solutions\n","    })\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    #if i >= 5: # Limit to the first few\n","    #   break\n","\n","print(f\"Code generation complete for the first 20 entries with {num_samples} sample each!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cf26481e"},"outputs":[],"source":["import json\n","\n","# Save the generated solutions to a JSONL file\n","output_file = \"generated_solutions_test_multisample.jsonl\"\n","with open(output_file, 'w') as f:\n","    for entry in generated_solutions:\n","        json.dump(entry, f)\n","        f.write('\\n')\n","\n","print(f\"Generated solutions saved to {output_file}\")"]},{"cell_type":"markdown","metadata":{"id":"4168a1ba"},"source":["## Set up evaluation environment\n","\n","Install necessary libraries and tools for evaluating the HumanEval solutions.\n"]},{"cell_type":"markdown","metadata":{"id":"733740b4"},"source":["## Load generated solutions\n","\n","Load the generated solutions from the JSONL file `generated_solutions_codellama_test.jsonl`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d6df130"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the generated solutions from the JSONL file\n","generated_solutions_df = pd.read_json(\"generated_solutions_test_multisample.jsonl\", lines=True)\n","\n","display(generated_solutions_df.head())"]},{"cell_type":"markdown","metadata":{"id":"180212e7"},"source":["## Run evaluation\n","\n","Use an appropriate evaluation script or library to evaluate the generated solutions against the HumanEval test cases.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVWXq4Qggp1s","collapsed":true},"outputs":[],"source":["import evaluate\n","import os\n","from collections import defaultdict\n","\n","# Set environment variable to allow code evaluation\n","os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n","\n","# Load the HumanEval evaluation metric\n","code_eval_metric = evaluate.load(\"code_eval\")\n","\n","# Prepare the predictions and references for evaluation\n","# predictions should be a list of lists, where each inner list contains the generated solutions for a single task\n","predictions = generated_solutions_df['solutions'].tolist()\n","\n","references = []\n","for i, example in enumerate(humaneval_dataset['test']):\n","    if i >= len(predictions):  # Limit references to the number of predictions\n","        break\n","    references.append(example['code'])\n","\n","\n","# Compute the evaluation results with multiple pass@k\n","evaluation_results = code_eval_metric.compute(\n","    references=references,\n","    predictions=predictions,\n","    k=[1, 5, 10, 15] # Specify the k values for pass@k\n",")\n","\n","display(evaluation_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22f7beac"},"outputs":[],"source":["display(f\"Pass@1 score: {evaluation_results[0]['pass@1']}\")\n","display(f\"Pass@5 score: {evaluation_results[0]['pass@5']}\")\n","display(f\"Pass@10 score: {evaluation_results[0]['pass@10']}\")\n","display(f\"Pass@15 score: {evaluation_results[0]['pass@15']}\")"]},{"cell_type":"markdown","metadata":{"id":"186cffe5"},"source":["## Final Results Summary\n","\n","Here are the evaluation results and a summary of the syntax analysis."]},{"cell_type":"code","metadata":{"id":"94945bd1"},"source":["import ast\n","import pandas as pd\n","\n","def analyze_syntax(code):\n","    try:\n","        ast.parse(code)\n","        return \"Syntax OK\"\n","    except SyntaxError as e:\n","        return f\"Syntax Error: {e}\"\n","\n","# Assuming generated_solutions_df is already loaded from the JSONL file\n","if 'generated_solutions_df' not in locals():\n","    try:\n","        generated_solutions_df = pd.read_json(\"generated_solutions_test_multisample.jsonl\", lines=True)\n","    except FileNotFoundError:\n","        display(\"Error: generated_solutions_test_multisample.jsonl not found. Please run the code generation cells first.\")\n","        generated_solutions_df = pd.DataFrame(columns=['task_id', 'solutions'])\n","\n","\n","syntax_analysis_results = []\n","for solutions_list in generated_solutions_df['solutions']:\n","    task_syntax_results = [analyze_syntax(solution) for solution in solutions_list]\n","    syntax_analysis_results.append(task_syntax_results)\n","\n","# Add the syntax analysis results to the DataFrame\n","generated_solutions_df['syntax_analysis'] = syntax_analysis_results\n","\n","# Display the DataFrame with syntax analysis results for the first solution of each task\n","display(\"\\n--- Syntax Analysis Summary (First Solution per Task) ---\")\n","display(generated_solutions_df[['task_id', 'syntax_analysis']].head())\n","\n","# Save the detailed AST analysis to a CSV file\n","# Saving analysis for all solutions for completeness\n","all_syntax_results = []\n","for index, row in generated_solutions_df.iterrows():\n","    for sol_index, result in enumerate(row['syntax_analysis']):\n","        all_syntax_results.append({\n","            'task_id': row['task_id'],\n","            'solution_index': sol_index,\n","            'syntax_result': result,\n","            'solution_code': row['solutions'][sol_index] # Include the solution code\n","        })\n","\n","syntax_analysis_detailed_df = pd.DataFrame(all_syntax_results)\n","output_csv_file = \"syntax_analysis_detailed.csv\"\n","syntax_analysis_detailed_df.to_csv(output_csv_file, index=False)\n","\n","display(f\"\\nDetailed syntax analysis saved to {output_csv_file}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"28e3b6ae"},"source":["total_solutions = 0\n","syntax_ok_solutions = 0\n","\n","for task_results in generated_solutions_df['syntax_analysis']:\n","    total_solutions += len(task_results)\n","    syntax_ok_solutions += task_results.count('Syntax OK')\n","\n","display(f\"Total number of generated solutions: {total_solutions}\")\n","display(f\"Number of solutions with OK syntax: {syntax_ok_solutions}\")\n","display(f\"Number of solutions with Syntax Errors: {total_solutions - syntax_ok_solutions}\")"],"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1jVCmAbp_w_QljgUCB2aI-Evxc6ZkCxut","timestamp":1761053911002},{"file_id":"1QdhchkD1YEloKSPemysMZmyuIlegZ1It","timestamp":1760963323768},{"file_id":"1kpoTj1HDIwjW_pFPWxPuVqvxgoovQMvT","timestamp":1760946157880},{"file_id":"1qGDryuIoqSdpwz5SbVoSztTHExX2G1zy","timestamp":1760248670893}],"machine_shape":"hm","authorship_tag":"ABX9TyM6KpMXdbFhzxno+6GngxOX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}